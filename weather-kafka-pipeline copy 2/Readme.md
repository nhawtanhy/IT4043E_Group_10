

ğŸŒ¦ï¸ Weather Data Pipeline

Kafka â†’ Spark Streaming â†’ Elasticsearch â†’ Kibana
Kafka â†’ HDFS â†’ Spark Batch â†’ Elasticsearch

This system collects weather data from an API, sends it into Kafka, processes it in real time with Spark Streaming, stores enriched results in Elasticsearch for visualization, and simultaneously writes raw data to HDFS for deeper batch analytics.

â¸»

ğŸ“¦ System Components
	â€¢	Zookeeper â€” coordinates Kafka
	â€¢	Kafka Broker â€” receives and stores weather messages
	â€¢	Weather Producer â€” generates real-time weather events
	â€¢	Spark Streamer â€” real-time processing + publishes to Elasticsearch
	â€¢	HDFS (Namenode + Datanode) â€” stores raw Parquet files
	â€¢	Spark HDFS Streamer (optional) â€” Kafka â†’ HDFS Parquet writer
	â€¢	Spark Batch â€” daily batch ETL and aggregations
	â€¢	Elasticsearch â€” stores streaming + batch results
	â€¢	Kibana â€” visualization dashboard

â¸»

ğŸš€ 1. Build Services Before Running

Spark Streaming

docker compose build spark-streamer

Spark Batch

docker compose build spark-batch

Spark HDFS Streamer (optional)

docker compose build spark-hdfs-streamer


â¸»

ğŸš€ 2. Start the System

2.1 Core Services: Zookeeper + Kafka

docker compose up -d zookeeper
docker compose up -d kafka

2.2 Elasticsearch + Kibana

docker compose up -d elasticsearch
docker compose up -d kibana

2.3 Weather Producer

docker compose up -d weather-producer
docker logs -f weather-producer

2.4 Spark Streaming (real-time)

docker compose up -d spark-streamer
docker logs -f spark-streamer
docker exec -it spark-streamer rm -rf /checkpoint

Expected log:

========== BATCH X ==========
[OK] Saved batch X â†’ Elasticsearch


â¸»

ğŸ—‚ï¸ 3. Start HDFS

3.1 Namenode

docker compose up -d namenode

If this is your first run:

docker exec -it namenode hdfs namenode -format

3.2 Datanode

docker compose up -d datanode

3.3 Check HDFS

docker exec -it namenode hdfs dfsadmin -report
docker exec -it namenode hdfs dfs -ls /


â¸»

ğŸ“ 4. Spark HDFS Streamer (Kafka â†’ HDFS)

Run this if you want to store raw data in HDFS:

docker compose up -d spark-hdfs-streamer
docker logs -f spark-hdfs-streamer

Check for output files:

docker exec -it namenode hdfs dfs -ls /weather/parquet


â¸»

ğŸ“Š 5. Spark Batch ETL (HDFS â†’ Elasticsearch)

Run the batch job manually:

docker compose run spark-batch spark-submit /app/spark_batch.py

Or run as a service:

docker compose up -d spark-batch
docker logs -f spark-batch

Batch results are written into:

weather_agg


â¸»

ğŸ” 6. Quick Inspection Commands

Kafka â€” view messages

docker exec -it kafka kafka-console-consumer \
  --bootstrap-server kafka:9092 \
  --topic weather_raw \
  --from-beginning \
  --max-messages 10

Kafka â€” check offsets

docker exec -it kafka kafka-run-class kafka.tools.GetOffsetShell \
  --broker-list kafka:9092 \
  --topic weather_raw

HDFS â€” list files

docker exec -it namenode hdfs dfs -ls -R /weather

Elasticsearch â€” test index

curl http://localhost:9201/weather/_search?pretty
curl http://localhost:9201/weather_agg/_search?pretty

Spark logs

docker logs -f spark-streamer
docker logs -f spark-hdfs-streamer
docker logs -f spark-batch


â¸»

ğŸ›‘ 7. Stop the Entire System

Stop containers:

docker compose down

Stop and remove volumes:

docker compose down -v


